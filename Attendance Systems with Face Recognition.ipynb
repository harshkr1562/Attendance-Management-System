{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c19dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from matplotlib.pyplot import matplotlib\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Convolution2D,MaxPooling2D,ZeroPadding2D,Flatten,Dense,Activation,Dropout,Input\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras import Sequential\n",
    "\n",
    "from mtcnn import MTCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c84ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "names = os.listdir(r\"C:\\Users\\hp\\Desktop\\data\\dataset1\")\n",
    "\n",
    "filepath=r'C:\\Users\\hp\\Desktop\\data\\dataset1'\n",
    "\n",
    "filenames = []\n",
    "\n",
    "for name in names:\n",
    "    for file in os.listdir(os.path.join(filepath,name)):\n",
    "        \n",
    "        filenames.append(os.path.join(filepath,name,file))\n",
    "        \n",
    "\n",
    "pickle.dump(filenames,open('Filenames.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "266c3bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mark Zuckerberg'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)\n",
    "filenames[300].split('\\\\')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941a1543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\anacond\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\zero_padding2d.py:72: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e97f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(r\"C:\\Users\\hp\\Downloads\\vgg_face_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54918910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\anacond\\Lib\\site-packages\\keras\\src\\models\\model.py:342: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# pickle.dump(model,open('Model.pkl','wb'))\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb7f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c22433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def facial_feature_extractor(img_path,model):\n",
    "#     img=face_extractor(img_path)\n",
    "    img = image.load_img(img_path,target_size=(224,224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    expanded_img = np.expand_dims(img_array,axis=0)\n",
    "    preprocessed_img = preprocess_input(expanded_img)\n",
    "\n",
    "    result = model.predict(preprocessed_img).flatten()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6444ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_extractor(img_path):\n",
    "    detector = MTCNN()\n",
    "    # load img -> face detection\n",
    "    img = cv2.imread(img_path)\n",
    "    results = detector.detect_faces(img)\n",
    "\n",
    "    x,y,width,height = results[0]['box']\n",
    "\n",
    "    img = img[y:y+height+25,x:x+width+25]\n",
    "    \n",
    "    return img\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db2c3736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00768a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# features = []\n",
    "\n",
    "# for file in tqdm(filenames):\n",
    "#     features.append(facial_feature_extractor(file,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02dca75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# pickle.dump(features,open('features.pkl','wb'))\n",
    "\n",
    "features = pickle.load(open(r\"C:\\Users\\hp\\Desktop\\data\\features.pkl\",'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c564a5b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2622"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09843ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity,euclidean_distances\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c90907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step\n",
      "WARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000241BBE1B4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
      "0.6928188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = MTCNN()\n",
    "# load img -> face detectmyimg.jpgion\n",
    "sample_img = cv2.imread(r\"C:\\Users\\hp\\Desktop\\data\\test_images\\myimg.jpg\")\n",
    "results = detector.detect_faces(sample_img)\n",
    "\n",
    "x,y,width,height = results[0]['box']\n",
    "\n",
    "face = sample_img[y:y+height,x:x+width]\n",
    "\n",
    "#  extract its features\n",
    "image = Image.fromarray(face)\n",
    "image = image.resize((224,224))\n",
    "\n",
    "face_array = np.asarray(image)\n",
    "\n",
    "face_array = face_array.astype('float32')\n",
    "\n",
    "expanded_img = np.expand_dims(face_array,axis=0)\n",
    "preprocessed_img = preprocess_input(expanded_img)\n",
    "result = model.predict(preprocessed_img).flatten()\n",
    "#print(result)\n",
    "#print(result.shape)\n",
    "# find the cosine distance of current image with all the 8655 features\n",
    "similarity = []\n",
    "for i in range(len(features)):\n",
    "    similarity.append(cosine_similarity(result.reshape(1,-1),features[i].reshape(1,-1))[0][0])\n",
    "\n",
    "index_pos = sorted(list(enumerate(similarity)),reverse=True,key=lambda x:x[1])[0][0]\n",
    "print(max(similarity))\n",
    "temp_img = cv2.imread(filenames[index_pos])\n",
    "cv2.imshow('output',temp_img)\n",
    "cv2.waitKey(0)\n",
    "# recommend that image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25191323",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\hp\\Desktop\\data\\attendencesheet\\name.csv.txt\"\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b455e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e57fe0ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
      "0.5514283\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 570ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "0.5792457\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    ret,frame=cap.read()\n",
    "    img=cv2.resize(frame,(224,224))\n",
    "    \n",
    "    \n",
    "    \n",
    "    detector=MTCNN()\n",
    "    sample_img = frame\n",
    "    results = detector.detect_faces(sample_img)\n",
    "    \n",
    "    x,y,w,h = results[0]['box']\n",
    "\n",
    "    face = sample_img[y:y+h,x:x+w]\n",
    "    \n",
    "    #  extract its features\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize((224,224))\n",
    "\n",
    "    face_array = np.asarray(image)\n",
    "\n",
    "    face_array = face_array.astype('float32')\n",
    "\n",
    "    expanded_img = np.expand_dims(face_array,axis=0)\n",
    "    preprocessed_img = preprocess_input(expanded_img)\n",
    "    result = model.predict(preprocessed_img).flatten()\n",
    "        \n",
    "    similarity = []\n",
    "    for i in range(len(features)):\n",
    "        similarity.append(cosine_similarity(result.reshape(1,-1),features[i].reshape(1,-1))[0][0])\n",
    "\n",
    "    index_pos = sorted(list(enumerate(similarity)),reverse=True,key=lambda x:x[1])[0][0]\n",
    "    cosine = sorted(list(enumerate(similarity)),reverse=True,key=lambda x:x[1])[0][1]\n",
    "    print(max(similarity))\n",
    "\n",
    "    if cosine>0.40:\n",
    "        temp_img = cv2.imread(filenames[index_pos])\n",
    "        temp_img=cv2.resize(temp_img,(224,224))\n",
    "        cv2.imshow('output',temp_img)\n",
    "\n",
    "\n",
    "        cv2.rectangle(frame,pt1=(x,y),pt2=(x+w,y+h),thickness=2,color=(0,255,0))\n",
    "        cv2.putText(frame,'Matched Succesfully [{}]'.format(filenames[index_pos].split('\\\\')[-2].upper()),(30,30),cv2.FONT_HERSHEY_TRIPLEX,1, (0,255,0), 1,cv2.LINE_AA)\n",
    "        with open(file_path, mode='a', newline='\\n') as file: \n",
    "            file.write('{}-{}\\n'.format(filenames[index_pos].split('\\\\')[-2].upper(),datetime.now()))\n",
    "                \n",
    "    else:\n",
    "        cv2.rectangle(frame,pt1=(x,y),pt2=(x+w,y+h),thickness=2,color=(0,0,255))\n",
    "        cv2.putText(frame,'Not Matched ',(30,30),cv2.FONT_HERSHEY_TRIPLEX,1, (0,0,255), 1,cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('input',frame)\n",
    "    \n",
    "    \n",
    "   \n",
    "            \n",
    "    cv2.imshow('input',frame)\n",
    "    \n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()       \n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "198c9eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(img):\n",
    "    detector = MTCNN()\n",
    "    # load img -> facload_img detection\n",
    "    sample_img = cv2.imread(img)\n",
    "    results = detector.detect_faces(sample_img)\n",
    "\n",
    "    x,y,width,height = results[0]['box']\n",
    "\n",
    "    face = sample_img[y:y+height,x:x+width]\n",
    "\n",
    "    #  extract its features\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize((224,224))\n",
    "\n",
    "    face_array = np.asarray(image)\n",
    "\n",
    "    face_array = face_array.astype('float32')\n",
    "\n",
    "    expanded_img = np.expand_dims(face_array,axis=0)\n",
    "    preprocessed_img = preprocess_input(expanded_img)\n",
    "    result = model.predict(preprocessed_img).flatten()\n",
    "    #print(result)\n",
    "    #print(result.shape)\n",
    "    # find the cosine distance of current image with all the 8655 features\n",
    "    similarity = []\n",
    "    for i in range(len(features)):\n",
    "        similarity.append(cosine_similarity(result.reshape(1,-1),features[i].reshape(1,-1))[0][0])\n",
    "\n",
    "    index_pos = sorted(list(enumerate(similarity)),reverse=True,key=lambda x:x[1])[0][0]\n",
    "#     print(max(similarity))\n",
    "    temp_img = cv2.imread(filenames[index_pos])\n",
    "    temp_img=cv2.resize(temp_img,(224,224))\n",
    "    \n",
    "    cv2.putText(temp_img,'Matched !! [{}]'.format(filenames[index_pos].split('\\\\')[-2].upper()),(10,10),cv2.FONT_HERSHEY_TRIPLEX,0.35, (0,255,0), 1,cv2.LINE_AA)\n",
    "\n",
    "    return temp_img\n",
    "\n",
    "#     cv2.imshow('output',temp_img)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fb52c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_people(img):\n",
    "    file_path = r\"C:\\Users\\hp\\Desktop\\data\\attendencesheet\\name.csv.txt\"\n",
    "    cap=cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        \n",
    "    \n",
    "        ret,frame=cap.read()\n",
    "        img=cv2.resize(frame,(224,224))\n",
    "\n",
    "\n",
    "\n",
    "        detector=MTCNN()\n",
    "        sample_img = frame\n",
    "        results = detector.detect_faces(sample_img)\n",
    "\n",
    "        x,y,w,h = results[0]['box']\n",
    "\n",
    "        face = sample_img[y:y+h,x:x+w]\n",
    "\n",
    "        #  extract its features\n",
    "        image = Image.fromarray(face)\n",
    "        image = image.resize((224,224))\n",
    "\n",
    "        face_array = np.asarray(image)\n",
    "\n",
    "        face_array = face_array.astype('float32')\n",
    "\n",
    "        expanded_img = np.expand_dims(face_array,axis=0)\n",
    "        preprocessed_img = preprocess_input(expanded_img)\n",
    "        result = model.predict(preprocessed_img).flatten()\n",
    "\n",
    "\n",
    "        similarity = []\n",
    "        for i in range(len(features)):\n",
    "            similarity.append(cosine_similarity(result.reshape(1,-1),features[i].reshape(1,-1))[0][0])\n",
    "\n",
    "        index_pos = sorted(list(enumerate(similarity)),reverse=True,key=lambda x:x[1])[0][0]\n",
    "        cosine = sorted(list(enumerate(similarity)),reverse=True,key=lambda x:x[1])[0][1]\n",
    "        print(max(similarity))\n",
    "\n",
    "        if cosine>0.5:\n",
    "            temp_img = cv2.imread(filenames[index_pos])\n",
    "            temp_img=cv2.resize(temp_img,(224,224))\n",
    "#             cv2.imshow('output',temp_img)\n",
    "\n",
    "\n",
    "            cv2.rectangle(frame,pt1=(x,y),pt2=(x+w,y+h),thickness=2,color=(0,255,0))\n",
    "            cv2.putText(frame,'Matched Succesfully [{}]'.format(filenames[index_pos].split('\\\\')[1].upper()),(30,30),cv2.FONT_HERSHEY_TRIPLEX,1, (0,255,0), 1,cv2.LINE_AA)\n",
    "            frame=Image.fromarray(frame)\n",
    "            return frame\n",
    "#             return ('Matched Succesfully [{}]'.format(filenames[index_pos].split('\\\\')[1].upper())) \n",
    "#             with open(file_path, mode='a', newline='\\n') as file: \n",
    "#                 file.write('{}-{}\\n'.format(filenames[qindex_pos].split('\\\\')[1].upper(),datetime.now()))\n",
    "\n",
    "\n",
    "        else:\n",
    "            cv2.rectangle(frame,pt1=(x,y),pt2=(x+w,y+h),thickness=2,color=(0,0,255))\n",
    "            cv2.putText(frame,'Not Matched ',(30,30),cv2.FONT_HERSHEY_TRIPLEX,1, (0,0,255), 1,cv2.LINE_AA)\n",
    "            frame=Image.fromarray(frame)\n",
    "            return frame\n",
    "            \n",
    "\n",
    "#             cv2.imshow('input',frame)\n",
    "            return 'Not Matched '\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         cv2.imshow('input',frame)\n",
    "\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()       \n",
    "    cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b6e64",
   "metadata": {},
   "source": [
    "## Gradio App on Video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caf71449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35204d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "interface1=gr.Interface(\n",
    "    fn=identify_people,\n",
    "    inputs=gr.Image(sources=[\"webcam\"]),\n",
    "    outputs=gr.Image(type='numpy',image_mode='RGB'),\n",
    "    title=\"Attendance Systems with Face Recognition\",\n",
    "    description='Open Camera By Clicking Start and Capture a Photo For Face Verification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed07e544",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 4.27.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://1a4d2ae7b4dd266894.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1a4d2ae7b4dd266894.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interface1.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622a512",
   "metadata": {},
   "source": [
    "## gradio app on Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8612bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab5e9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interface=gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type='filepath'),\n",
    "    outputs=gr.Image(type='filepath'),\n",
    "    title=\"Attendance Systems with Face Recognition\",\n",
    "    description='Open Camera By Clicking Start and Capture a Photo For Face Verification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f38f87e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "Running on public URL: https://3a8d95ff9aac2e978b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n"
     ]
    }
   ],
   "source": [
    "interface.launch(inline=False,share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "955711e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "interface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a333fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f42224f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
